<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Lonely Tech Planet</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="weiming&apos;s personal blog/website">
<meta property="og:type" content="website">
<meta property="og:title" content="Lonely Tech Planet">
<meta property="og:url" content="http://coretech.org/index.html">
<meta property="og:site_name" content="Lonely Tech Planet">
<meta property="og:description" content="weiming&apos;s personal blog/website">
<meta property="og:locale" content="English">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Lonely Tech Planet">
<meta name="twitter:description" content="weiming&apos;s personal blog/website">
  
    <link rel="alternate" href="/atom.xml" title="Lonely Tech Planet" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Lonely Tech Planet</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">love tech love life</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://coretech.org"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-On-Availability-for-Blockchain-Based-Systems" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/02/26/On-Availability-for-Blockchain-Based-Systems/" class="article-date">
  <time datetime="2018-02-26T15:52:28.000Z" itemprop="datePublished">2018-02-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/26/On-Availability-for-Blockchain-Based-Systems/">On Availability for Blockchain-Based Systems</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Report2<br>On Availability for Blockchain-Based Systems<br>yangwm9@mail2.sysu.edu.cn</p>
<p>Duration: 2018/02/24 to 2018/02/26<br>Introduction<br>●transactions can record data about any kind of application domain, and can deploy and execute user-defined scripts<br>●understanding the dependability properties supported by blockchains is crucial<br>●we should measure the availability of mainstream blockchains in terms of their responsiveness<br>○define the notion of transaction commit needed by applications<br>○A commit requires instead that some peers create a sequence of blocks<br>●clients cannot expect a time for their transaction to commit<br>●proof-of-work blockchains impact the dependability of systems<br>●network reordering plays a important role in causing significant delays<br>●transactions can remain uncommitted and even stuck in the transaction pool</p>
<p>Background<br>●Every participant holds a local copy of the ledger and runs a network client that relays transactions to the entire network<br>●asymmetric cryptography: owner with private key can create and sign transactions and others can verify it by corresponding public key<br>●all participants are in consensus about which transactions have been successfully made in case of double spend problem<br>○proof-of-work<br>○proof-of-stake<br>●A miner should find a random number and compute a hash sum over this block which must be smaller than a certain target value. Only brute-force search. More over, target value will decrease which make the computation more difficult.<br>●A fork problem is two miners may occasionally find a new block at the same time and propagate it. However, every miner should work from the currently longest chain. Thus, we need the appropriate number of confirmation blocks to be waited. “51% attack” is difficult.<br>●mining pool may be dangerous because attackers can make use of it.<br>●Ethereum provides a mechanism called “gas price”</p>
<p>Commit of Bitcoin Transactions<br>●transactions can experience delay that they must arrive in-order<br>●orphan transactions should wait for their parents arriving first<br>●locktimes can affect the transaction delay<br>●Data Collection Methodology<br>○modify bctd and change the mempool to log every incoming transaction<br>○3 categories in pool: Straight-accept, rejected, orphaned(out-of-order arrival)<br>●Inclusion Time of Bitcoin Transactions<br>○the fraction of orphans in the second experiment is higher<br>○the majority reason for reject is the transactions are already in mempool<br>○transaction fees and locktimes may be two main factors<br>○It is unlikely that lower transaction fees are a cause for dalayed commit<br>○orphans never had locktimes beyond the observation window which shows that locktimes are not likely to be a decisive factor<br>Commit of Ethereum Transactions<br>●Ethereum Transaction Handling<br>○it is impossible to know whether a transaction that is invalid in some state of the system will never be valid in a later state.<br>○uncles in Ethereum<br>●Data Collection and Basic Statistics<br>○modify the geth client in Ethereum<br>○record the time because no timestamp(for mining)<br>●From First Inclusion to Commit<br>○transactions that were in uncles need to go back to transaction pool<br>○Ethereum recommends to wait for 11 confirmations after block inclusion before a transaction is committed permanently<br>○growing faction of transactions that have to wait longer for a commit<br>●Impact of User-defined Gas Price and Limit<br>○gas price and the maximum gas offered for including a transaction<br>○the higher the gas price in a given band, the less likely long delays<br>○can not find correlation in any direction between maximum gas and delay<br>●Impact of network delays<br>○a transaction with a nonce n+1 cannot be included into the blockchain unless there is an already included transaction with nonce n.<br>○in-order and out-of-order arrival of transactions experiments<br>○commit delay for out-of-order is doubled<br>○work connectivity may have negatively impacted transaction propagation</p>
<p>○netImpact of the Block Gas Limit in Ethereum<br>●a second form of limit —-&gt; gas limit per block. it is defined by the network of miners and applies to the sum of gas consumed by all transactions in a block<br>●prevent the DoS attacks by limiting the amount of computation per block<br>●most of the function that are currently in use are not computationally intensive<br>●many contracts would not have been deployable while the block gas limit in place</p>
<p>Transaction Abort In Ethereum<br>●some options to achieve an effect that is similar to an explicit abort<br>○more transaction fees or retry the same Tx<br>●scenarios for abortion:<br>○1. a transaction is not included in the usual period of time<br>○2. a client changes its mind and decide to abort<br>○3. a transaction is in indefinite pending state due to insufficient funds<br>●scenario 1:<br>○most transactions were accepted by the network<br>○6/10 with either 0 or 0.2 <em> mr were accepted<br>○2/10 with 0.1 </em> mr were accepted<br>●scenario 2:<br>○set the time out value to be 3 mins<br>○a much higher percentage of transactions were not included in a block<br>○there may be many reasons for this distribution: processing time, broadcast delays or no preference is given to transaction with higher fee<br>●scenario 3:<br>○submit Tx2 first and then Tx1 so that 2 Txs can be in the pool<br>○Tx2 is in a indefintie state because it can be accpeted later<br>○the experiment is very sucessful</p>
<p>Advantage of this paper<br>●authors of this paper extract all the factor from the reason for commit delay like gas price, gas limit, network and so on. Each experiment in this paper should consider all the factors and can not be affected by other factors when measuring a effect a factor.<br>●This paper used a large amount of numbers, graphs and plots so that it is very convincing and trusty.<br>●it is very clever to achieve an effect that is similar to an explicit abort by a way that the system or user can issue a competing transaction from the same source account or submitting another transaction that the field in it contain the same data as before<br>●Also it is really cool to create a situation that a transaction in a pool is in a indefinite state in experiment 3 by submitting the transaction with a larger nonce and then submitting the transaction with the smaller one.<br>●this paper found that network reordering can impact dramatically commit times and even counterbalance the effects of transaction fees and gas price.<br>Disadvantage of this paper<br>●The experiment on Bitcoin was conduct only in 2016 and 2017, each lasted for 25hours. In fact, we can collect more data so that the result can be clearer and more accurate<br>●A obvious limitation of their work is that  they do not know the locktimes of those orphans that were not included in the blockchain by the end of their observation period.<br>●There may still be confounding factors in their study they can not control. For example, they conducted their experiments and collected the data only for Australia, and the nodes in Australia are somewhat connected to outside Australia.<br>●They also could not determine the Internet routing constellation that the Bitcoin network is exposed to.<br>●The paper did a good job in analyzing the factor of user-defined gas price and limit but they could not find a strong correlation in any direction between maximum gas and commit delay. This can be an open question for all the researchers specializing in blockchain system.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://coretech.org/2018/02/26/On-Availability-for-Blockchain-Based-Systems/" data-id="cje4elpte0000twdj155i507j" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-report-for-Efficient-Estimation-of-Word-Representations-in-Vector-Space" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/02/23/report-for-Efficient-Estimation-of-Word-Representations-in-Vector-Space/" class="article-date">
  <time datetime="2018-02-23T15:19:51.000Z" itemprop="datePublished">2018-02-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/23/report-for-Efficient-Estimation-of-Word-Representations-in-Vector-Space/">report for Efficient Estimation of Word Representations in Vector Space</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Report1"><a href="#Report1" class="headerlink" title="Report1"></a>Report1</h1><h2 id="Efficient-Estimation-of-Word-Representations-in-Vector-Space"><a href="#Efficient-Estimation-of-Word-Representations-in-Vector-Space" class="headerlink" title="Efficient Estimation of Word Representations in Vector Space"></a>Efficient Estimation of Word Representations in Vector Space</h2><h3 id="yangwm9-mail2-sysu-edu-cn"><a href="#yangwm9-mail2-sysu-edu-cn" class="headerlink" title="yangwm9@mail2.sysu.edu.cn"></a>yangwm9@mail2.sysu.edu.cn</h3><h3 id="Duration-2018-02-21-to-2018-02-28"><a href="#Duration-2018-02-21-to-2018-02-28" class="headerlink" title="Duration: 2018/02/21 to 2018/02/28"></a>Duration: 2018/02/21 to 2018/02/28</h3><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul>
<li>Previous NLP models like N-grams are at their limits in many tasks.</li>
<li>Goals: introduce techniques used for learning high-quality word vectors from huge data sets with billions of words with following features<ul>
<li>not restricted to a few hundred of millions of words</li>
<li>dimensionality of word vectors between 50-100</li>
<li>words can have multiple degree of similarity</li>
<li>word vectors can perform simple algebraic operations</li>
<li>test for measuring both syntactic and semantic regularities</li>
<li>training time and accuracy(dimensionality and amount of data)</li>
</ul>
</li>
<li>Previous Work: NNLM with single layer, word vectors are first learned by NNLM, then the resulting word vectors reversely train the NNLM. The technique introduced in this paper focus on the first step.</li>
</ul>
<h3 id="Model-Architectures"><a href="#Model-Architectures" class="headerlink" title="Model Architectures"></a>Model Architectures</h3><ul>
<li>Feedforward Neural Network Language Model (NNLM)<ul>
<li>4 parts: input, projection, hidden and output layers.</li>
<li>1-of-V coding for N previous words in input layer(one-hot representation)</li>
<li>Projection layer has dimensionality of N x D with a shared projection matrix</li>
<li>hidden layer is used to compute probability distribution over all the words in the vocabulary, resulting in an output layer with dimensionality V</li>
<li>the computational complexity per each training example is Q = N <em> D + N </em> D <em> H + H </em> V where most of the complexity is caused by N <em> D </em> H</li>
<li>hierarchical softmax and Huffman binary tree for speedup</li>
</ul>
</li>
<li>Recurrent Neural Net Language Model (RNNLM)<ul>
<li>overcome some certain limitations of NNLM</li>
<li>RNN model does not have a projection layer</li>
<li>complexity: Q = H <em> H + H </em> V where most  complexity comes from H * H</li>
</ul>
</li>
<li>Parallel Training of Neural Networks<ul>
<li>a large-scale distributed framework DistBelief used to run multiple replicas of the same model in parallel</li>
<li>mini-batch asynchronous gradient descent with Adagrad</li>
</ul>
</li>
</ul>
<h3 id="New-Log-linear-Models"><a href="#New-Log-linear-Models" class="headerlink" title="New Log-linear Models"></a>New Log-linear Models</h3><ul>
<li>continuous word vectors are learned using simple model</li>
<li>N-gram NNLM is trained on top of representations of words</li>
<li>speed up the process of training for word representations</li>
<li>Continuous Bag-of-Words Model<ul>
<li>the hidden layer is removed and the projection layer is shared for all words</li>
<li>the order of words does not influence the projection</li>
<li>use future and history words as input continuously</li>
<li>complexity: Q = N <em> D + D </em> log2(V)</li>
</ul>
</li>
<li>Continuous Skip-gram Model<ul>
<li>reverse process of CBOW</li>
<li>maximize classification of a word based on another word in same sentence</li>
<li>predict words within a certain range before and after the current word</li>
<li>complexity: Q = C <em> (D + D </em> log2(V)) where C is the disctance of th words</li>
</ul>
</li>
</ul>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><ul>
<li>What’s the answer of the question: vector X = vector(biggest) - vector(big) + vector(small) ?</li>
<li>The model can perform simple algebraic operations with the representation of words and it is possible to find the correct answer (smallest) using this model.</li>
<li>Task Description<ul>
<li>First find a list of similar word pairs manually</li>
<li>train the word pairs to be vector representations</li>
<li>connecting 2 word pairs by the formula above, it forms the question and predicts the answer, then calculate the accuracy</li>
<li>the vocabulary size to 1 million most frequent words. However, this is only for English and how about Chinese? After deleting the duplicate words in Chinese, I think the size will less than 1 million in a large scale</li>
</ul>
</li>
<li>Maximization of Accuracy<ul>
<li>adding more dimension or adding more training data either increases accuracy and training time, but the extent is different respectively</li>
<li>In some case, increasing the dimension is better than training data while the performance is better by increasing training data in some case</li>
<li>When the dimension is over 300, the improvement of increasing the dimension is diminishing.</li>
</ul>
</li>
<li>Comparison of Model Architectures<ul>
<li>In the same training words and dimension, Skip-gram model performs excellent both semantically and syntactically but CBOW model performs badly semantically. What’s the reason?</li>
<li>training a model using 1 epoch gives comparable or better results than iterating over the same data for 3 epochs, which shows that it is enough for us to train the data with 1 epoch.</li>
<li>training time of Skip-gram is related to the parameter C and the training time is approximately 3 times longer than CBOW.</li>
<li>the training time of both CBOW and Skip-gram is in proportion to either the amount of training words or vector dimensionality</li>
</ul>
</li>
<li>Large Scale Parallel Training of Models<ul>
<li>In a large scale corpus, it is impossible to train the high dimensional data by NNLM because the training time and cost are too high</li>
<li>In a large scale corpus and high vector dimensionality, the CBOW model is more accurate than Skip-gram syntactically </li>
</ul>
</li>
<li>Microsoft Research Sentence Completion Challenge<ul>
<li>the author turns this task into a task that calculates the probability of a sentence(for each of the 5 words, choose the maximum probability of the sentence after calculation)</li>
</ul>
</li>
</ul>
<h3 id="Some-notes-and-opinions-about-this-paper"><a href="#Some-notes-and-opinions-about-this-paper" class="headerlink" title="Some notes and opinions about this paper"></a>Some notes and opinions about this paper</h3><ul>
<li>all the NLP models including NNLM, RNNLM, CBOW, Skip-gram are trained by machine learning algorithms called stochastic gradient descent and backpropagation.</li>
<li>training time and accuracy rate are related to vector dimensionality and the mount of training data.</li>
<li>this paper does not include the multi-word entities also it is restricted to English. When it comes to Chinese, the dimension and training data will vary.</li>
<li>the initial learning rate is 0.025 which is too small and it decreases linearly.</li>
<li>the architecture of composition: Skip-gram and RNNLMs is a good way to train data.</li>
<li>word vectors can be used in many NLP applications and tasks like sentiment analysis and paraphrase detection.</li>
<li>window size: skip-gram is usually at 10 and CBOW at 5.</li>
<li>CBOW is faster than Skip-gram when the the amount of training data is in a large scale and Skip-gram have advantage in training rare or not frequent words.</li>
<li>hierarchical Huffman level softmax is better than the traditional ones.</li>
<li>the speedup algorithm can be negative sample, huffman, softmax or the combination of these algorithms. </li>
<li>3 major word vectors NN training model: glove, word2vec and fasttext</li>
</ul>
<h3 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h3><ul>
<li>why does CBOW model perform worse than Skip-gram semantically?</li>
<li>what if the text and words are in Chinese?</li>
<li>why should the hidden layer vectors average the sum of the vectors as hidden vector? What about log function and others?</li>
<li>how to understand the weights between hidden layer and output layer?</li>
<li>why is SGD is best for the training model? Can it be improved by applying the mini batch gradient descent algorithm?</li>
<li>how to define the loss function which means how close is the output vector to the ground-truth vector of the label(desired vector)?</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://coretech.org/2018/02/23/report-for-Efficient-Estimation-of-Word-Representations-in-Vector-Space/" data-id="cje4elptn0001twdj53nlfq9z" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/28/hello-world/" class="article-date">
  <time datetime="2018-01-28T03:15:57.051Z" itemprop="datePublished">2018-01-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/28/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>第一篇博文</p>
<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://coretech.org/2018/01/28/hello-world/" data-id="cje4elpu00002twdjtga6sqfu" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/02/26/On-Availability-for-Blockchain-Based-Systems/">On Availability for Blockchain-Based Systems</a>
          </li>
        
          <li>
            <a href="/2018/02/23/report-for-Efficient-Estimation-of-Word-Representations-in-Vector-Space/">report for Efficient Estimation of Word Representations in Vector Space</a>
          </li>
        
          <li>
            <a href="/2018/01/28/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Jimmy Yang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>