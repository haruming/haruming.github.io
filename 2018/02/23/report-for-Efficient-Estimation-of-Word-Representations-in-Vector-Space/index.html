<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>report for Efficient Estimation of Word Representations in Vector Space | Lonely Tech Planet</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Report1Efficient Estimation of Word Representations in Vector Spaceyangwm9@mail2.sysu.edu.cn Duration: 2018/02/21 to 2018/02/28Introduction    ● Previous NLP models like N-grams are at their limits in">
<meta property="og:type" content="article">
<meta property="og:title" content="report for Efficient Estimation of Word Representations in Vector Space">
<meta property="og:url" content="http://coretech.org/2018/02/23/report-for-Efficient-Estimation-of-Word-Representations-in-Vector-Space/index.html">
<meta property="og:site_name" content="Lonely Tech Planet">
<meta property="og:description" content="Report1Efficient Estimation of Word Representations in Vector Spaceyangwm9@mail2.sysu.edu.cn Duration: 2018/02/21 to 2018/02/28Introduction    ● Previous NLP models like N-grams are at their limits in">
<meta property="og:locale" content="English">
<meta property="og:updated_time" content="2018-02-23T15:24:09.392Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="report for Efficient Estimation of Word Representations in Vector Space">
<meta name="twitter:description" content="Report1Efficient Estimation of Word Representations in Vector Spaceyangwm9@mail2.sysu.edu.cn Duration: 2018/02/21 to 2018/02/28Introduction    ● Previous NLP models like N-grams are at their limits in">
  
    <link rel="alternate" href="/atom.xml" title="Lonely Tech Planet" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Lonely Tech Planet</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">love tech love life</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://coretech.org"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-report-for-Efficient-Estimation-of-Word-Representations-in-Vector-Space" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/02/23/report-for-Efficient-Estimation-of-Word-Representations-in-Vector-Space/" class="article-date">
  <time datetime="2018-02-23T15:19:51.000Z" itemprop="datePublished">2018-02-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      report for Efficient Estimation of Word Representations in Vector Space
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Report1<br>Efficient Estimation of Word Representations in Vector Space<br>yangwm9@mail2.sysu.edu.cn</p>
<p>Duration: 2018/02/21 to 2018/02/28<br>Introduction<br>    ● Previous NLP models like N-grams are at their limits in many tasks.<br>    ● Goals: introduce techniques used for learning high-quality word vectors from huge data sets with billions of words with following features<br>        ○ not restricted to a few hundred of millions of words<br>        ○ dimensionality of word vectors between 50-100<br>        ○ words can have multiple degree of similarity<br>        ○ word vectors can perform simple algebraic operations<br>        ○ test for measuring both syntactic and semantic regularities<br>        ○ training time and accuracy(dimensionality and amount of data)<br>    ● Previous Work: NNLM with single layer, word vectors are first learned by NNLM, then the resulting word vectors reversely train the NNLM. The technique introduced in this paper focus on the first step.</p>
<p>Model Architectures<br>    ● Feedforward Neural Network Language Model (NNLM)<br>        ○ 4 parts: input, projection, hidden and output layers.<br>        ○ 1-of-V coding for N previous words in input layer(one-hot representation)<br>        ○ Projection layer has dimensionality of N x D with a shared projection matrix<br>        ○ hidden layer is used to compute probability distribution over all the words in the vocabulary, resulting in an output layer with dimensionality V<br>        ○ the computational complexity per each training example is Q = N <em> D + N </em> D <em> H + H </em> V where most of the complexity is caused by N <em> D </em> H<br>        ○ hierarchical softmax and Huffman binary tree for speedup<br>    ● Recurrent Neural Net Language Model (RNNLM)<br>        ○ overcome some certain limitations of NNLM<br>        ○ RNN model does not have a projection layer<br>        ○ complexity: Q = H <em> H + H </em> V where most  complexity comes from H * H<br>    ● Parallel Training of Neural Networks<br>        ○ a large-scale distributed framework DistBelief used to run multiple replicas of the same model in parallel<br>        ○ mini-batch asynchronous gradient descent with Adagrad</p>
<p>New Log-linear Models<br>    1, continuous word vectors are learned using simple model<br>    2, N-gram NNLM is trained on top of representations of words<br>    3, speed up the process of training for word representations<br>    ● Continuous Bag-of-Words Model<br>        ○ the hidden layer is removed and the projection layer is shared for all words<br>        ○ the order of words does not influence the projection<br>        ○ use future and history words as input continuously<br>        ○ complexity: Q = N <em> D + D </em> log2(V)<br>    ● Continuous Skip-gram Model<br>        ○ reverse process of CBOW<br>        ○ maximize classification of a word based on another word in same sentence<br>        ○ predict words within a certain range before and after the current word<br>        ○ complexity: Q = C <em> (D + D </em> log2(V)) where C is the disctance of th words</p>
<p>Results<br>    What’s the answer of the question: vector X = vector(biggest) - vector(big) + vector(small) ?<br>    The model can perform simple algebraic operations with the representation of words and it is possible to find the correct answer (smallest) using this model.<br>    ● Task Description<br>    ○ First find a list of similar word pairs manually<br>    ○ train the word pairs to be vector representations<br>    ○ connecting 2 word pairs by the formula above, it forms the question and predicts the answer, then calculate the accuracy<br>    ○ the vocabulary size to 1 million most frequent words. However, this is only for English and how about Chinese? After deleting the duplicate words in Chinese, I think the size will less than 1 million in a large scale<br>    ● Maximization of Accuracy<br>        ○ adding more dimension or adding more training data either increases accuracy and training time, but the extent is different respectively<br>        ○ In some case, increasing the dimension is better than training data while the performance is better by increasing training data in some case<br>        ○ When the dimension is over 300, the improvement of increasing the dimension is diminishing.<br>    ● Comparison of Model Architectures<br>        ○ In the same training words and dimension, Skip-gram model performs excellent both semantically and syntactically but CBOW model performs badly semantically. What’s the reason?<br>        ○ training a model using 1 epoch gives comparable or better results than iterating over the same data for 3 epochs, which shows that it is enough for us to train the data with 1 epoch.<br>        ○ training time of Skip-gram is related to the parameter C and the training time is approximately 3 times longer than CBOW.<br>        ○ the training time of both CBOW and Skip-gram is in proportion to either the amount of training words or vector dimensionality<br>    ● Large Scale Parallel Training of Models<br>        ○ In a large scale corpus, it is impossible to train the high dimensional data by NNLM because the training time and cost are too high<br>        ○ In a large scale corpus and high vector dimensionality, the CBOW model is more accurate than Skip-gram syntactically<br>    ● Microsoft Research Sentence Completion Challenge<br>        ○the author turns this task into a task that calculates the probability of a sentence(for each of the 5 words, choose the maximum probability of the sentence after calculation)</p>
<p>Some notes and opinions about this paper<br>    ● all the NLP models including NNLM, RNNLM, CBOW, Skip-gram are trained by machine learning algorithms called stochastic gradient descent and backpropagation.<br>    ● training time and accuracy rate are related to vector dimensionality and the mount of training data.<br>    ● this paper does not include the multi-word entities also it is restricted to English. When it comes to Chinese, the dimension and training data will vary.<br>    ● the initial learning rate is 0.025 which is too small and it decreases linearly.<br>    ● the architecture of composition: Skip-gram and RNNLMs is a good way to train data.<br>    ● word vectors can be used in many NLP applications and tasks like sentiment analysis and paraphrase detection.<br>    ● window size: skip-gram is usually at 10 and CBOW at 5.<br>    ● CBOW is faster than Skip-gram when the the amount of training data is in a large scale and Skip-gram have advantage in training rare or not frequent words.<br>    ● hierarchical Huffman level softmax is better than the traditional ones.<br>    ●the speedup algorithm can be negative sample, huffman, softmax or the combination of these algorithms.<br>    ● 3 major word vectors NN training model: glove, word2vec and fasttext</p>
<p>Questions<br>    ● why does CBOW model perform worse than Skip-gram semantically?<br>    ● what if the text and words are in Chinese?<br>    ● why should the hidden layer vectors average the sum of the vectors as hidden vector? What about log function and others?<br>    ● how to understand the weights between hidden layer and output layer?<br>    ● why is SGD is best for the training model? Can it be improved by applying the mini batch gradient descent algorithm?<br>    ● how to define the loss function which means how close is the output vector to the ground-truth vector of the label(desired vector)?</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://coretech.org/2018/02/23/report-for-Efficient-Estimation-of-Word-Representations-in-Vector-Space/" data-id="cje03ymg70000scdjboufugn2" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2018/01/28/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/02/23/report-for-Efficient-Estimation-of-Word-Representations-in-Vector-Space/">report for Efficient Estimation of Word Representations in Vector Space</a>
          </li>
        
          <li>
            <a href="/2018/01/28/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Jimmy Yang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>